from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any

# --- Request Schemas ---

class Message(BaseModel):
    """Represents a message in a chat conversation."""
    role: str = Field(..., description="The role of the author of this message (e.g., 'system', 'user', 'assistant').")
    content: str = Field(..., description="The content of the message.")
    # GigaChat might support additional fields like 'name', 'function_call', etc.
    # Add them here if needed and supported by GigaChat API
    # name: Optional[str] = None

class ChatCompletionRequest(BaseModel):
    """Represents the request body for a chat completion."""
    model: str = Field(..., description="ID of the model to use.")
    messages: List[Message] = Field(..., description="A list of messages comprising the conversation so far.")
    temperature: Optional[float] = Field(0.7, ge=0.0, le=2.0, description="What sampling temperature to use.")
    max_tokens: Optional[int] = Field(None, gt=0, description="The maximum number of tokens to generate in the chat completion.")
    stream: Optional[bool] = Field(False, description="If set, partial message deltas will be sent.")
    # Add other parameters if GigaChat API supports them and you need them,
    # like top_p, n, stop, presence_penalty, frequency_penalty, logit_bias, user
    # top_p: Optional[float] = Field(None, ge=0.0, le=1.0)
    # n: Optional[int] = Field(None, ge=1)


# --- Response Schemas (for non-streaming) ---

class Usage(BaseModel):
    """Represents token usage statistics."""
    prompt_tokens: Optional[int] = None
    completion_tokens: Optional[int] = None
    total_tokens: Optional[int] = None

class ChatCompletionMessage(BaseModel):
    """Represents a message returned in a chat completion response."""
    role: str = Field(..., description="The role of the author of this message.")
    content: str = Field(..., description="The content of the message.")

class ChatCompletionChoice(BaseModel):
    """Represents a single completion choice."""
    index: int = Field(..., description="The index of the choice in the list of choices.")
    message: ChatCompletionMessage = Field(..., description="The message generated by the model.")
    finish_reason: Optional[str] = Field(None, description="The reason the model finished generating tokens.")

class ChatCompletionResponse(BaseModel):
    """Represents the response body for a non-streaming chat completion."""
    id: str = Field(..., description="A unique identifier for the chat completion.")
    object: str = Field("chat.completion", description="The object type, which is always chat.completion.")
    created: int = Field(..., description="The Unix timestamp (in seconds) of when the chat completion was created.")
    model: str = Field(..., description="The model used for the chat completion.")
    choices: List[ChatCompletionChoice] = Field(..., description="A list of chat completion choices.")
    usage: Optional[Usage] = Field(None, description="Usage statistics for the completion.")

# --- Response Schemas (for streaming chunks) ---
# Note: Streaming chunks have a slightly different structure for the 'delta' field
# and send choices incrementally.

class ChatCompletionChunkDelta(BaseModel):
    """Represents the incremental delta in a streaming chunk."""
    role: Optional[str] = Field(None, description="The role of the author of this chunk.")
    content: Optional[str] = Field(None, description="The content of the chunk.")
    # Add other potential delta fields like 'function_call' if supported by GigaChat streaming

class ChatCompletionChunkChoice(BaseModel):
    """Represents a single choice in a streaming chunk."""
    index: int = Field(..., description="The index of the choice in the list of choices.")
    delta: ChatCompletionChunkDelta = Field(..., description="The incremental content delta for this chunk.")
    finish_reason: Optional[str] = Field(None, description="The reason the model finished generating tokens in this chunk (present in the final chunk).")
    
    # Add error field for stream errors if GigaChat includes it in chunks
    error: Optional[Dict[str, Any]] = Field(None, description="Details about an error if one occurred during streaming.")


class ChatCompletionChunk(BaseModel):
    """Represents a single streaming chat completion chunk."""
    id: str = Field(..., description="A unique identifier for the chat completion (consistent across chunks).")
    object: str = Field("chat.completion.chunk", description="The object type, which is always chat.completion.chunk.")
    created: int = Field(..., description="The Unix timestamp (in seconds) of when the chat completion was created (consistent across chunks).")
    model: str = Field(..., description="The model used for the chat completion.")
    choices: List[ChatCompletionChunkChoice] = Field(..., description="A list of chat completion choices (typically one).")
    # Usage is typically not included in streaming chunks, but might appear in the final one depending on API.
    # Usage: Optional[Usage] = Field(None, description="Usage statistics for the completion (may be in final chunk).")